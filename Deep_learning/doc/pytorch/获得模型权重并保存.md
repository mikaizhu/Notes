[TOC]

## 模型的保存

**假如训练了如下模型，然后将模型保存**

```
import torch
import os
import torch.nn as nn

class PolynomialRegression(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = nn.Linear(4, 1)
    def forward(self, t):
        return self.fc(t)
     
model = PolynomialRegression()
torch.save(model, 'model.pt')
```

## 模型的加载

要想读取保存的模型，必须先定义一个一样的类对象，**注意这里必须继承nn.Module，否者有很多方法都使用不了**

```
class PolynomialRegression(nn.Module):
    pass
```

然后再load进来

```
model = torch.load('model.pt')
```

## 获得模型的权重(scheme1)

**假如原先定义的网络如下：**

```
class PolynomialRegression(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = nn.Linear(4, 1)
    def forward(self, t):
        return self.fc(t)
```

可以看到上面网络只有一层，且名字为fc，输入为4，输出为1，所以只有4的1次方个weight和一个bias

**查看网络层名字：**

```
for name in model.state_dict():
   print(name)
   
#fc.weight
#fc.bias
```

**利用名字获得模型的权重：**

```
model.state_dict()['fc.weight']
#tensor([[-1.0885, -1.9960,  3.1411, -0.0486]])

model.state_dict()['fc.bias']
#tensor([0.0749])
```

## **获得模型权重(scheme2)**

**假如现在模型如下：**

```
class PolynomialRegression(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(4, 1)
        self.fc2 = nn.Linear(2, 8)
        self.fc3 = nn.Conv2d(in_channels=2, out_channels=4, kernel_size=2)
        
model = PolynomialRegression()
```

**打印模型框架：**

```
for layer in model.modules():
    print('-'*40)
    print(layer)
    print('-'*40)
```

```
----------------------------------------
PolynomialRegression(
  (fc1): Linear(in_features=4, out_features=1, bias=True)
  (fc2): Linear(in_features=2, out_features=8, bias=True)
  (fc3): Conv2d(2, 4, kernel_size=(2, 2), stride=(1, 1))
)
----------------------------------------
----------------------------------------
Linear(in_features=4, out_features=1, bias=True)
----------------------------------------
----------------------------------------
Linear(in_features=2, out_features=8, bias=True)
----------------------------------------
----------------------------------------
Conv2d(2, 4, kernel_size=(2, 2), stride=(1, 1))
----------------------------------------
```

**配合isinstance**

- **isinstance(object, int)，注意第一个是对象，第二个参数是类型**

```
for layer in model.modules():
    if isinstance(layer, nn.Linear):
        print(layer)
        
#Linear(in_features=4, out_features=1, bias=True)
#Linear(in_features=2, out_features=8, bias=True)
```

```
for layer in model.modules():
    if isinstance(layer, nn.Linear):
        print(layer.weight)
        
output:
Parameter containing:
tensor([[0.1343, 0.0439, 0.4955, 0.1172]], requires_grad=True)
Parameter containing:
tensor([[-0.4455, -0.4027],
        [ 0.6785, -0.6589],
        [-0.0344,  0.0896],
        [-0.6437, -0.4206],
        [ 0.2207, -0.4967],
        [-0.3603, -0.0050],
        [-0.6982, -0.5954],
        [ 0.3078, -0.3168]], requires_grad=True)
```

```
for layer in model.modules():
    if isinstance(layer, nn.Linear):
        print(layer.bias)
```

```
Parameter containing:
tensor([-0.1030], requires_grad=True)
Parameter containing:
tensor([-0.2683,  0.4835, -0.6370,  0.6083,  0.2070, -0.5874,  0.0956,  0.0300],
       requires_grad=True)
```

- **model.named_parameters()**

```
for name, param in model.named_parameters():
    if name == 'fc3.weight':
        print(param.detach().numpy())
```

**可以使用上面方式查看某一层的参数**

```
parm={}
for name,parameters in resnet18.named_parameters():
    parm[name]=parameters.detach().numpy()
```

## 保存网络的权重参数

**还可以直接使用字典保存模型的参数**

1. **Shceme1**

```
torch.save(net.state_dict(),PATH)
model_dict=model.load_state_dict(torch.load(PATH))
```

2. **Scheme2**

```
parm={}
for name,parameters in resnet18.named_parameters():
    parm[name]=parameters.detach().numpy()
```

## 参考教程

1. https://blog.csdn.net/happyday_d/article/details/88974361

2. https://zhuanlan.zhihu.com/p/67184419