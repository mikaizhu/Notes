参考教程：

- https://bailingnan.github.io/post/pytorch-chang-yong-dai-ma-duan/#%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F

- https://www.jianshu.com/p/9643cba47655
- https://pytorch.org/docs/stable/optim.html 官网文档说明

**方式1 手动修改:**

```
model = net()
LR = 0.01
optimizer = Adam(model.parameters(),lr = LR)
lr_list = []
for epoch in range(100):
    if epoch % 5 == 0:
        for p in optimizer.param_groups:
            p['lr'] *= 0.9
    lr_list.append(optimizer.state_dict()['param_groups'][0]['lr'])
plt.plot(range(100),lr_list,color = 'r')
```

**方式2 使用内置的函数修改，格式如下**

```
model = ClassificationHead(l).cuda()
optimizer = optim.Adam(model.parameters(), lr=lr)
lf = nn.CrossEntropyLoss()
scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda t: t / 2)


for epoch in range(EPOCH):
    model.train()
    for feature, label in train_loader:
        feature = feature.float().cuda()
        label = label.cuda()
        optimizer.zero_grad()
        preds = model(feature)
        loss = lf(preds, label)
        loss.backward()
        optimizer.step()
        
    print(f'epoch:{epoch:2} loss:{loss:4f}')

    model.eval()
    with torch.no_grad():
        total_acc = 0
        total_num = 0
        for feature, label in test_loader:
            feature = feature.float().cuda()
            label = label.cuda()
            total_acc += model(feature).argmax(dim=1).eq(label).sum().item()
            total_num += label.shape[0]
        print('val_acc:{:4f}'.format(total_acc/total_num))
    
    scheduler.step()
```

另外比较常用的一种：

每个一定的epoch，lr会自动乘以gamma

```
scheduler = lr_scheduler.StepLR(optimizer,step_size=5,gamma = 0.8)
lr_list.append(optimizer.state_dict()['param_groups'][0]['lr'])
```

