**同步爬虫的弊端：**

什么是同步爬虫？下面代码URL是一个个爬取的，只有等一个数据爬取完，才能爬第二个，这样效率太低了。

```
import time

def get_page(url):
    print(f'正在爬取{url}')
    time.sleep(1)
    return 0

urls = ['url1', 'url2', 'url3', 'url4']

star = time.time()
for url in urls:
    res = get_page(url)
end = time.time()

print(f'run time {end - star}')
```

```
正在爬取url1
正在爬取url2
正在爬取url3
正在爬取url4
run time 4.0070960521698
```

**异步爬虫的方式：**

- **多线程和多进程**
  - 好处：可以为相关阻塞的操作单独开启线程或者进程，阻塞操作可以异步执行
  - 弊端：进程开启和关闭，可能反而会导致爬取速度变慢
- **线程池和进程池(适当的使用)**
  - 好处：我们可以降低系统对进程或者线程创建和销毁的一个频率，从而很好的降低系统的开销。
  - 弊端：池中的线程或进程的数量是有上限的。

**使用方法很简单：**

```
from multiprocessing.dummy import Pool
import time

def get_page(url):
    print(f'正在爬取{url}')
    time.sleep(1)
    return 0

urls = ['url1', 'url2', 'url3', 'url4']

pool = Pool(4) # 实例化一个线程池对象
star = time.time()
pool.map(get_page, urls) # 比较耗时的操作都可以写入到函数中，get Page为比较耗时的函数，urls为处理的列表
end = time.time()

print(f'run time {end - star}')
```

```
# 可以发现运行时间变成1s了

正在爬取url1正在爬取url2
正在爬取url3

正在爬取url4
1.0080268383026123
```

- **单线程加异步协程(推荐使用)**

**使用的基本方式：**

```
import asyncio
import time

# async修饰的函数，调用后会返回一个协程对象
async def get_page(url):
    print(f'正在爬取{url}')
    time.sleep(1)
    return 0
    
# 使用一个变量接收这个协程对象
c = get_page('url1')

# 创建一个事件循环对象
loop = asyncio.get_event_loop()

# 创建一个事件循环对象
loop = asyncio.get_event_loop()
```

**如果出现错误：RuntimeError: This event loop is already running，是因为jupyter notebook中不准许使用协程**

如果想查看更多有关于协程的说明，请参考：https://www.bilibili.com/video/BV1ut4y1B7CX?p=44

