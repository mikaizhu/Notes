[TOC]

## åŒºå—é“¾çˆ¬è™«

çˆ¬å–çš„ç½‘å€ï¼šhttps://dappradar.com/thundercore/games/galaxy-blocks

**TODO**ï¼š

- [x] Part1 æ•°æ®çˆ¬å–
- [x] Part2 æ•°æ®å¤„ç†
- [x] Part3 æ•°æ®å¯è§†åŒ–
  - [ ] å…¶å®è¿™é‡Œå¯è§†åŒ–è¿˜å¯ä»¥å¯¹allæ•°æ®è¿›è¡Œæ“ä½œï¼Œæ¯”å¦‚çœ‹ä¸€äº›èŠ‚å‡æ—¥æˆ–è€…å‘¨æœ«çš„çŠ¶å†µã€‚
  - [ ] å‚è€ƒ [æ—¶é—´åºåˆ—æ¯”èµ›çš„å¯è§†åŒ–](https://github.com/ChuanyuXue/The-Purchase-and-Redemption-Forecast-Challenge-baseline/blob/master/01.%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E4%B8%8E%E5%88%86%E6%9E%90.ipynb)
- [x] Part4 æ¨¡å‹è®­ç»ƒ
  - [x] å°è¯•è¿›è¡Œé¢„æµ‹ç»“æœå¯è§†åŒ–
  - [ ] å°†åæ ‡æ¢æˆæ—¥æœŸï¼Œæœ‰ç‚¹ç´¯ç°åœ¨ä¸æƒ³å¼„
  - [x] ä»‹ç»ä¸‹æ¨¡å‹ä¼˜ç¼ºç‚¹å’Œå¯¹è¿™ä¸ªé—®é¢˜çš„å¯è¡Œæ€§
  - [ ] å†ä½¿ç”¨ä¸€ä¸ªæ¨¡å‹ï¼Œå¯¹æ•ˆæœè¿›è¡Œå¯¹æ¯”ï¼Ÿ
- [x] Part5 ç»“æœåˆ†æ
  - [ ] è¦ä¸è¦ä»‹ç»ä¸‹æ¨¡å‹ï¼Œæ„Ÿè§‰ç¯‡å¹…è¦å¾ˆé•¿ğŸ˜“
  - [x] åˆ†æä¸‹ç»“æœ
  - [ ] åˆ†æç»“æœè¿›è¡Œè¡¥å……

### Part1 æ•°æ®çˆ¬å–



**è¦çˆ¬å–çš„æ•°æ®å¦‚ä¸‹ï¼Œæ•°æ®æ˜¯å¯ä»¥åŠ¨æ€äº¤äº’çš„ï¼Œè¿™ç§æ•°æ®ä¸€èˆ¬ç›´æ¥ä½¿ç”¨requestsæ¨¡å—æŠ“å–ä¸åˆ°ï¼Œéœ€è¦åˆ†æç½‘é¡µï¼Œæ‰¾åˆ°æ•°æ®çš„æ¥æºã€‚**

![image.png](http://ww1.sinaimg.cn/large/005KJzqrgy1gp22fym7hkj30s40ju40u.jpg)

**è¿™ç§åŠ¨æ€çš„æ•°æ®äº¤äº’ï¼Œä¸€èˆ¬éƒ½æ˜¯ä½¿ç”¨ajaxæŠ€æœ¯ï¼Œåé¢æœ‰jsonæ ¼å¼çš„æ•°æ®è¿›è¡Œäº¤äº’ã€‚å…ˆå¯¹ajaxè¿›è¡Œåˆ†æï¼Œä½¿ç”¨chromeæµè§ˆå™¨è‡ªå¸¦çš„æŠ“åŒ…åŠŸèƒ½ï¼ŒæŠ“åŒ…ç»“æœå¦‚å›¾ï¼š**

![image.png](http://ww1.sinaimg.cn/large/005KJzqrgy1gp22nl2ub6j327i15oao6.jpg)

**ç‚¹å‡»é‡Œé¢çš„ä¸‰ä¸ªæ–‡ä»¶ï¼Œå¯ä»¥æ‰¾åˆ°æ•°æ®è¯·æ±‚çš„URLå’Œæ•°æ®ç±»å‹ä¸ºjson**

![image.png](http://ww1.sinaimg.cn/large/005KJzqrgy1gp22pm2xsjj31120jkn0e.jpg)

**æ¥ä¸‹æ¥å¯ä»¥ç¼–å†™ä»£ç å¯¹æ•°æ®è¿›è¡Œçˆ¬å–ï¼Œæ‰¾åˆ°æ•°æ®æ¥æºä»¥åå°±æ¯”è¾ƒæ–¹ä¾¿äº†ï¼Œç›´æ¥ä½¿ç”¨pythonä¸­çš„requestsæ¨¡å—ï¼Œå¯¹jsonæ•°æ®è¿›è¡Œè¯·æ±‚ã€‚**

**ç›´æ¥æ‰“å¼€ä¸Šé¢çš„URLï¼Œå¾—åˆ°çš„ç¡®å®æ˜¯jsonæ•°æ®ï¼Œç»“æœå¦‚ä¸‹ï¼š**

![image.png](http://ww1.sinaimg.cn/large/005KJzqrgy1gp2325ce1rj31l607qdi7.jpg)

**ä»£ç æµ‹è¯•ï¼Œçœ‹çœ‹èƒ½ä¸èƒ½ç›´æ¥æŠ“å–æ•°æ®ï¼š**

```
import requests

week_url = 'https://dappradar.com/v2/api/dapp/thundercore/games/galaxy-blocks/chart/week'
month_url = 'https://dappradar.com/v2/api/dapp/thundercore/games/galaxy-blocks/chart/month'
all_url = 'https://dappradar.com/v2/api/dapp/thundercore/games/galaxy-blocks/chart/all'

week = requests.get(week_url)

with open('res.txt', 'w') as f:
    f.write(week.text)
```

è¿è¡Œä»£ç åï¼Œå‘ç°æŠ“åˆ°çš„å¹¶ä¸æ˜¯jsonæ•°æ®ã€‚çœ‹çœ‹æ˜¯ä¸æ˜¯è¯·æ±‚å‚æ•°é—®é¢˜

**ä¿®æ”¹ä»£ç ï¼š**

```
import requests

week_url = 'https://dappradar.com/v2/api/dapp/thundercore/games/galaxy-blocks/chart/week'
month_url = 'https://dappradar.com/v2/api/dapp/thundercore/games/galaxy-blocks/chart/month'
all_url = 'https://dappradar.com/v2/api/dapp/thundercore/games/galaxy-blocks/chart/all'

params = {
    ':authority': 'dappradar.com',
    ':path': '/v2/api/dapp/thundercore/games/galaxy-blocks/chart/week',
}

headers = {
    'cookie': '__cfduid=d87b8e46401fad8e04b0395c6f31f22911617082335; _ga=GA1.2.189733543.1617082335; _gid=GA1.2.645825448.1617082335; _rdt_uuid=1617082346122.934071ca-529e-4361-80b4-96f037f64020; _fbp=fb.1.1617082347737.332184334; _hjid=a4521e83-66b7-4bb4-8658-95f6b9a93041; _gat=1; _hp2_id.3928182892=%7B%22userId%22%3A%224342861640682969%22%2C%22pageviewId%22%3A%224126843480895195%22%2C%22sessionId%22%3A%224259085859779402%22%2C%22identity%22%3Anull%2C%22trackerVersion%22%3A%224.0%22%7D; _hjIncludedInSessionSample=1; _hjAbsoluteSessionInProgress=1; _hp2_ses_props.3928182892=%7B%22ts%22%3A1617097371792%2C%22d%22%3A%22dappradar.com%22%2C%22h%22%3A%22%2Fthundercore%2Fgames%2Fgalaxy-blocks%22%7D',
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 11_1_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.192 Safari/537.36',
}

week = requests.get(week_url, params=params, headers=headers)
```

**ç»“æœå¦‚ä¸‹ï¼Œå‘ç°è¿™æ¬¡å¯¹äº†ï¼ŒåŸå› æ˜¯æ²¡æœ‰åŠ cookieå’Œå…¶ä»–å‚æ•°ï¼š**

```
week.json()

# ç»“æœå¦‚ä¸‹
{'series': [{'name': 'Users',
   'data': [12198, 11266, 10695, 11356, 10321, 10584]},
  {'name': 'Volume', 'data': [44540, 41603, 41973, 43661, 37459, 38982]},
  {'name': 'Transactions',
   'data': [15581, 14929, 14558, 15165, 14204, 14457]}],
 'xaxis': [1616976000000,
  1616889600000,
  1616803200000,
  1616716800000,
  1616630400000,
  1616544000000]}
```



**æ•°æ®æŠ“å–éƒ¨åˆ†å®Œæ•´ä»£ç å¦‚ä¸‹ï¼š**

```
import requests
import pickle

week_url = 'https://dappradar.com/v2/api/dapp/thundercore/games/galaxy-blocks/chart/week'
month_url = 'https://dappradar.com/v2/api/dapp/thundercore/games/galaxy-blocks/chart/month'
all_url = 'https://dappradar.com/v2/api/dapp/thundercore/games/galaxy-blocks/chart/all'

# è¯·æ±‚å‚æ•°è®¾ç½®
params = {
    ':authority': 'dappradar.com',
    ':path': '/v2/api/dapp/thundercore/games/galaxy-blocks/chart/week',
}

headers = {
    'cookie': '__cfduid=d87b8e46401fad8e04b0395c6f31f22911617082335; _ga=GA1.2.189733543.1617082335; _gid=GA1.2.645825448.1617082335; _rdt_uuid=1617082346122.934071ca-529e-4361-80b4-96f037f64020; _fbp=fb.1.1617082347737.332184334; _hjid=a4521e83-66b7-4bb4-8658-95f6b9a93041; _gat=1; _hp2_id.3928182892=%7B%22userId%22%3A%224342861640682969%22%2C%22pageviewId%22%3A%224126843480895195%22%2C%22sessionId%22%3A%224259085859779402%22%2C%22identity%22%3Anull%2C%22trackerVersion%22%3A%224.0%22%7D; _hjIncludedInSessionSample=1; _hjAbsoluteSessionInProgress=1; _hp2_ses_props.3928182892=%7B%22ts%22%3A1617097371792%2C%22d%22%3A%22dappradar.com%22%2C%22h%22%3A%22%2Fthundercore%2Fgames%2Fgalaxy-blocks%22%7D',
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 11_1_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.192 Safari/537.36',
}

# å¯¹jsonæ•°æ®è¿›è¡ŒæŠ“å–
week = requests.get(week_url, params=params, headers=headers)
month = requests.get(month_url, params=params, headers=headers)
all_ = requests.get(all_url, params=params, headers=headers)

# ä½¿ç”¨pickleæ¨¡å—æ•°æ®ä¿å­˜ï¼Œç”±äºæ•°æ®æ˜¯äºŒè¿›åˆ¶ï¼Œæ‰€ä»¥ä½¿ç”¨wbæ¨¡å¼
pickle.dump(week.json(), open('./week.pkl', 'wb'))
pickle.dump(month.json(), open('./month.pkl', 'wb'))
pickle.dump(all_.json(), open('./all_.pkl', 'wb'))
```

**è¯»å–å®Œjsonæ–‡ä»¶åï¼Œå‘ç°jsonæ•°æ®ä¸­å¹¶æ²¡æœ‰æºå¸¦æ—¥æœŸè¿™ä¸ªæ•°æ®ï¼Œæ‰€ä»¥éœ€è¦å¯¹ä»£ç è¿›è¡Œä¿®æ”¹ï¼Œæ‰¾åˆ°æ—¥æœŸçš„æ•°æ®ä½ç½®ã€‚**

**è¿™é‡Œjsonæ•°æ®ä¸­å¹¶æ²¡æœ‰ä¼ è¾“æ—¥æœŸæ•°æ®ï¼Œæ‰€ä»¥æ„Ÿè§‰å¾ˆå¥‡æ€ªï¼Œä¸è¿‡ä»–çš„æ—¥æœŸæ˜¯æŒ‰ä¸€å¤©ä¸€å¤©æ¥çš„ï¼Œæ‰€ä»¥å¯ä»¥æ‰‹åŠ¨ç”Ÿæˆæ—¥æœŸ**

### Part2 æ•°æ®å¤„ç†

**è¿™é‡Œæœ‰ä¸ªå°ç»†èŠ‚ï¼Œä»”ç»†è§‚å¯Ÿå¯¹åº”æ—¶é—´å’Œjsonçš„æ•°æ®å¯ä»¥å‘ç°ï¼Œjsonçš„æ•°æ®ï¼Œå¹¶ä¸æ˜¯æŒ‰æ—¶é—´é¡ºåºæ¥çš„ã€‚å¦‚ä¸‹å›¾ä¸­24å·Userså¯¹åº”ä¸º10.6kï¼Œè€Œjsonæ•°æ®ä¸­ç¬¬ä¸€ä¸ªä¸º12.2k**

![image.png](http://ww1.sinaimg.cn/large/005KJzqrgy1gp26dfmct3j32740qowlw.jpg)



**axisä½œç”¨ä¸ºé¼ æ ‡å¯¹åº”åœ¨æµè§ˆå™¨çš„åæ ‡ï¼Œæ‰€ä»¥æ•°æ®çš„æ’åºåº”è¯¥å’Œåæ ‡æœ‰å…³ï¼Œä»å·¦åˆ°å³æ’åºã€‚**

**è§‚å¯Ÿæ•°æ®çš„ç»“æ„ï¼Œä»£ç å¦‚ä¸‹ï¼š**

```
import pickle

# å¯ä»¥å‘ç°ä¸ç®¡æ˜¯weekï¼Œè¿˜æ˜¯month ï¼Œallæ•°æ®ï¼Œå­—å…¸éƒ½æ˜¯è¿™ç§ç»“æ„ï¼Œæ‰€ä»¥å¯ä»¥å†™æˆå‡½æ•°çš„å½¢å¼
def data_process(data):
    Users, Volume, Transactions = [], [], []
    data = data['series']
    for i in data:
        if i['name'] == 'Users':
            Users.extend(i['data'])
        elif i['name'] == 'Volume':
            Volume.extend(i['data'])
        else:
            Transactions.extend(i['data'])
    return Users, Volume, Transactions

# ä½¿ç”¨pickleå¯¹æ•°æ®è¿›è¡Œè¯»å–
week = pickle.load(open('./week.pkl', 'rb'))
month = pickle.load(open('./month.pkl', 'rb'))
all_ = pickle.load(open('./all_.pkl', 'rb'))

# æ‰“å°ä¸‹æ•°æ®ç±»å‹
print(type(week), type(month), type(all_))

# å¼€å§‹æ•°æ®å¤„ç†
week_user, week_volume, week_transactions = data_process(week)
month_user, month_volume, month_transactions = data_process(month)
all_user, all_volume, all_transactions = data_process(all_)
```

**ä¿®æ”¹åçš„ä»£ç å¦‚ä¸‹ï¼š**

```
import pickle
import numpy as np

# å¯ä»¥å‘ç°ä¸ç®¡æ˜¯weekï¼Œè¿˜æ˜¯month ï¼Œallæ•°æ®ï¼Œå­—å…¸éƒ½æ˜¯è¿™ç§ç»“æ„ï¼Œæ‰€ä»¥å¯ä»¥å†™æˆå‡½æ•°çš„å½¢å¼
def data_process(data):
    Users, Volume, Transactions = [], [], []
    Position = data['xaxis']
    data = data['series']
    for i in data:
        if i['name'] == 'Users':
            Users.extend(i['data'])
        elif i['name'] == 'Volume':
            Volume.extend(i['data'])
        else:
            Transactions.extend(i['data'])
    
    return Users, Volume, Transactions, Position
    
week = pickle.load(open('./week.pkl', 'rb'))
month = pickle.load(open('./month.pkl', 'rb'))
all_ = pickle.load(open('./all_.pkl', 'rb'))

week_users, week_volume, week_transactions, week_position = data_process(week)
month_users, month_volume, month_transactions, month_position = data_process(month)
all_users, all_volume, all_transactions, all_position = data_process(all_)

# å¼€å§‹å¯¹æ•°æ®è¿›è¡Œæ’åº
def get_rank_data(users, volumn, transaction, position):
    dic = dict(zip(position, range(len(position))))
    p = []
    for i in sorted(dic):
        p.append(dic[i])
    users = np.asarray(users)[p].tolist()
    volumn = np.asarray(volumn)[p].tolist()
    transaction = np.asarray(transaction)[p].tolist()

    return users, volumn, transaction

week_users, week_volume, week_transactions = get_rank_data(week_users, week_volume, week_transactions, week_position)

month_users, month_volume, month_transactions = get_rank_data(month_users, month_volume, month_transactions, month_position)

all_users, all_volume, all_transactions = get_rank_data(all_users, all_volume, all_transactions, all_position)
```

**ä»”ç»†æ ¸å¯¹åï¼Œæ•°æ®æ²¡é”™äº†~**

### Part3 æ•°æ®å¯è§†åŒ–

**ç›®å‰åªæ˜¯ç®€å•åšäº†ä¸€äº›å¯è§†åŒ–ï½**

```
import datetime
import matplotlib.dates as mdates
import os

# è®¾ç½®ç»˜å›¾é£æ ¼
import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use('ggplot')

# è®¾ç½®ä¸­æ–‡å­—ç¬¦
import matplotlib
matplotlib.rcParams['font.sans-serif'] = ['SimHei']
matplotlib.rcParams['axes.unicode_minus']=False

# å¼€å§‹å°è£…æˆå‡½æ•°
def plot_img(users, volume, transactions, time='week'):
    '''
    time: æœ‰weekï¼Œmonthï¼Œ allä¸‰ç§æ ¼å¼ï¼Œæ˜¾ç¤ºæ—¶é—´é—´éš”ä¸ä¸€æ ·
    '''
    fig, ax = plt.subplots()
    interval = {'week':1, 'month':4, 'all':60}
    date2 = {'week':datetime.date(2021, 3, 25), 'month':datetime.date(2021, 3, 2), 'all':datetime.date(2019, 9, 17)}
    markers = {'week':'o', 'month':'2', 'all':'.'}

    # å¯ä»¥è®¾ç½®çš„å‚æ•°MONTHLY, WEEKLY, DAILY
    rule = mdates.rrulewrapper(mdates.DAILY, interval=interval[time]) # è®¾ç½®æ—¶é—´åˆ»åº¦, intervalä¸ºæ—¶é—´é—´éš”
    loc = mdates.RRuleLocator(rule) # å¯¹æ—¶é—´åˆ»åº¦è¿›è¡Œå°è£…
    dateFmt = mdates.DateFormatter('%b/%d')

    ax.xaxis.set_major_locator(loc)
    ax.xaxis.set_major_formatter(dateFmt)

    ax.tick_params(axis='both', direction='out', labelsize=10)

    date1 = datetime.date(2021, 3, 31) # ä¸ç®¡æ˜¯weekè¿˜æ˜¯monthï¼Œç»“æŸæ—¶é—´éƒ½æ˜¯ä¸€æ ·çš„ï¼Œåªæ˜¯èµ·å§‹æ—¶é—´ä¸ä¸€æ ·
    delta = datetime.timedelta(days=1)
    dates = mdates.drange(date2[time], date1, delta)

    ax.plot_date(dates, users, '-', alpha=0.5, marker=markers[time], label='Users')
    ax.plot_date(dates, volume, '-', alpha=0.5, marker=markers[time], label='Volume')
    ax.plot_date(dates, transactions, '-', alpha=0.5, marker=markers[time], label='Transactions')

    plt.legend() # æ·»åŠ å›¾ä¾‹
    plt.title(time) # æ·»åŠ æ ‡é¢˜
    fig.autofmt_xdate()
    img_name = time + '.jpg'
    if not os.path.exists('./picture'):
        os.mkdir('./picture')
    plt.savefig('./picture/' + img_name) # å›¾ç‰‡ä¿å­˜
    plt.show()
    
plot_img(week_users, week_volume, week_transactions, time='week')
plot_img(month_users, month_volume, month_transactions, time='month')
plot_img(all_users, all_volume, all_transactions, time='all')
```

![](./picture/week.jpg)

![](./picture/month.jpg)

![](./picture/all.jpg)



### Part4 æ—¶é—´åºåˆ—é¢„æµ‹

**ä»€ä¹ˆæ˜¯æ—¶é—´åºåˆ—é¢„æµ‹é—®é¢˜ï¼Ÿ**

æ—¶é—´åºåˆ—é¢„æµ‹é—®é¢˜æ˜¯ä¸€ç§è‡ªå›å½’é—®é¢˜ï¼Œè‡ªå›å½’å°±æ˜¯` Y(t) = a*Y(t-1) + b*Y(t-2)`ï¼Œå³ç°åœ¨çš„Yå€¼ï¼Œç”±è¿‡å»çš„Yå€¼ç¡®å®šã€‚

**ä¼ ç»Ÿçš„è‡ªå›å½’æ¨¡å‹æœ‰å“ªäº›ï¼Ÿ**

è‡ªå›å½’æ¨¡å‹ARï¼Œç§»åŠ¨å¹³å‡æ¨¡å‹ MAï¼Œ è‡ªå›å½’ç§»åŠ¨å¹³å‡æ¨¡å‹ ARMAç­‰ã€‚

**æ¨¡å‹å¯è¡Œæ€§åˆ†æï¼š**

å¯¹äºæ—¶é—´åºåˆ—ï¼Œæ™®é€šçš„dnnï¼Œcnnç¥ç»ç½‘ç»œå¹¶ä¸å¥½å¤„ç†è¿™ç§é—®é¢˜ï¼ŒLSTMæ˜¯RNNç¥ç»ç½‘ç»œçš„ä¸€ç§ï¼Œèƒ½å¤Ÿå¾ˆå¥½åœ°è§£å†³æ—¶é—´åºåˆ—ä¸­çš„è®°å¿†æ€§é—®é¢˜ã€‚æ‰€ä»¥è¿™é‡Œä½¿ç”¨LSTMå¯¹transactionè¿›è¡Œåˆ†æé¢„æµ‹ã€‚

**LSTM Pytorchä»£ç å®ç°**

- **æ•°æ®è¯»å–å’Œå¤„ç†éƒ¨åˆ†ï¼š**

```
# å¯¼å…¥å¿…è¦æ¨¡å—
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
import pickle
import gc
import datetime
import time
from sklearn.model_selection import train_test_split
import torch
import torch.nn as nn
from torch import optim
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm

# å¯ä»¥å‘ç°ä¸ç®¡æ˜¯weekï¼Œè¿˜æ˜¯month ï¼Œallæ•°æ®ï¼Œå­—å…¸éƒ½æ˜¯è¿™ç§ç»“æ„ï¼Œæ‰€ä»¥å¯ä»¥å†™æˆå‡½æ•°çš„å½¢å¼
def data_process(data):
    Users, Volume, Transactions = [], [], []
    Position = data['xaxis']
    data = data['series']
    for i in data:
        if i['name'] == 'Users':
            Users.extend(i['data'])
        elif i['name'] == 'Volume':
            Volume.extend(i['data'])
        else:
            Transactions.extend(i['data'])
    
    return Users, Volume, Transactions, Position

# å¯¹æ—¥æœŸè¿›è¡Œæ’åº
def get_rank_data(users, volumn, transaction, position):
    dic = dict(zip(position, range(len(position))))
    p = []
    for i in sorted(dic):
        p.append(dic[i])
    users = np.asarray(users)[p].tolist()
    volumn = np.asarray(volumn)[p].tolist()
    transaction = np.asarray(transaction)[p].tolist()

    return users, volumn, transaction

# æ•°æ®è¯»å–å’Œå¤„ç†
week = pickle.load(open('./data/week.pkl', 'rb'))
month = pickle.load(open('./data/month.pkl', 'rb'))
all_ = pickle.load(open('./data/all_.pkl', 'rb'))

week_users, week_volume, week_transactions, week_position = data_process(week)
month_users, month_volume, month_transactions, month_position = data_process(month)
all_users, all_volume, all_transactions, all_position = data_process(all_)

week_users, week_volume, week_transactions = get_rank_data(week_users, week_volume, week_transactions, week_position)
month_users, month_volume, month_transactions = get_rank_data(month_users, month_volume, month_transactions, month_position)
all_users, all_volume, all_transactions = get_rank_data(all_users, all_volume, all_transactions, all_position)

# æ²¡æœ‰ç”¨çš„å˜é‡å¯ä»¥åˆ æ‰ï¼Œé¿å…å ç”¨å†…å­˜
del week, month, all_, week_position, month_position, all_position; gc.collect()

# å› ä¸ºæ²¡æœ‰æ—¶é—´ï¼Œæ‰€ä»¥è¿™é‡Œé’ˆå¯¹æ•°æ®åˆ›å»ºæ—¶é—´
date = pd.date_range('2019-9-17', '2021-3-30', freq='D') # ç”Ÿæˆ30å·ä¸€å…±561ä¸ªæ•°æ®

# åˆ›å»ºpandaså¯¹è±¡
df = pd.DataFrame({
    'transactions':all_transactions,
    'date':date
})

df = df.set_index(df.date)
del df['date']

data = df.sort_index(ascending=True, axis=0) # æŒ‰æ—¶é—´è¿›è¡Œæ’åºï¼Œè™½ç„¶æ•°æ®å‰é¢å·²ç»å¤„ç†å¥½äº†

# è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„åˆ’åˆ†
train, test = train_test_split(data, test_size=0.2, shuffle=False) # å› ä¸ºé»˜è®¤shuffleæ˜¯æ‰“ä¹±ï¼Œè¿™é‡Œè®¾ç½®ä¸ºFalse
```

- **æ•°æ®è½¬æ¢ï¼š**

```
# æ•°æ®å½’ä¸€åŒ–ï¼Œä¸ç„¶lossé™ä¸ä¸‹æ¥
window = 7 # è®¾ç½®æ—¶é—´çª—å£ï¼Œæ„é€ è®­ç»ƒé›†å’Œæ ‡ç­¾

# æ„é€ æ•°æ®é›†æ˜¯ä¸ºäº†è¾“å…¥åˆ°LSTMä¸­
x_train, y_train = [], []
# çª—å£ä¸º30è¡¨ç¤ºï¼Œç”¨30é•¿åº¦çª—å£æ»‘åŠ¨ï¼Œå‰30ä¸ªæ ·æœ¬ä½œä¸ºç‰¹å¾ï¼Œå1ä¸ªæ ·æœ¬ä½œä¸ºæ ‡ç­¾
for i in range(window, len(train)):
    x_train.append(train.values[i-window:i].flatten())
    y_train.append(train.values[i].flatten())
    
x_test, y_test = [], []
# çª—å£ä¸º30è¡¨ç¤ºï¼Œç”¨30é•¿åº¦çª—å£æ»‘åŠ¨ï¼Œå‰30ä¸ªæ ·æœ¬ä½œä¸ºç‰¹å¾ï¼Œå1ä¸ªæ ·æœ¬ä½œä¸ºæ ‡ç­¾
for i in range(window, len(test)):
    x_test.append(test.values[i-window:i, 0].flatten())
    y_test.append(test.values[i, 0].flatten())
    
train_numpy = np.array(x_train)
train_mean = np.mean(train_numpy)
train_std  = np.std(train_numpy)
train_numpy = (train_numpy - train_mean) / train_std
train_tensor = torch.Tensor(train_numpy)

label_numpy = np.array(y_train)
label_mean = np.mean(label_numpy)
label_std  = np.std(label_numpy)
label_numpy = (label_numpy - label_mean) / label_std
train_labels = torch.Tensor(label_numpy)

test_numpy = np.array(x_test)
test_mean = np.mean(test_numpy)
test_std  = np.std(test_numpy)
test_numpy = (test_numpy - test_mean) / test_std
test_tensor = torch.Tensor(test_numpy)

label_numpy = np.array(y_test)
label_mean = np.mean(label_numpy)
label_std  = np.std(label_numpy)
label_numpy = (label_numpy - label_mean) / label_std
test_labels = torch.Tensor(label_numpy)
```

- **å¼€å§‹å»ºæ¨¡ï¼š**

```
class LSTM(nn.Module):
    def __init__(self):
        super(LSTM, self).__init__()
        
        self.lstm = nn.LSTM(
            input_size=1,   # è¾“å…¥å°ºå¯¸ä¸º 1ï¼Œè¡¨ç¤ºä¸€å¤©çš„æ•°æ®
            hidden_size=64,
            num_layers=1, 
            batch_first=True)
        
        self.out = nn.Sequential(
            nn.Linear(64,1))
        
    def forward(self, x):
        r_out, (h_n, h_c) = self.lstm(x, None)   # None è¡¨ç¤º hidden state ä¼šç”¨å…¨ 0 çš„ state
        out = self.out(r_out[:, -1, :])          # å–æœ€åä¸€å¤©ä½œä¸ºè¾“å‡º
        
        return out
class MyDataset(Dataset):
    def __init__(self, features, labels, transformer=True):
        super().__init__()
        if transformer:
            self.features = torch.tensor(features).float().unsqueeze(dim=-1)
            self.labels = torch.tensor(labels).float()
        else:
            self.features = features.float().squeeze().unsqueeze(dim=-1)
            self.labels = labels.float()
    
    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]
    
    def __len__(self):
        return len(self.labels)    
lr = 0.01
EPOCHS = 500
gamma = 0.5
step_size = 100

train_set = MyDataset(train_tensor, train_labels, transformer=False)
train_loader = DataLoader(train_set, batch_size=10, shuffle=True)
test_set = MyDataset(test_tensor, test_labels, transformer=False)
test_loader = DataLoader(test_set, batch_size=10, shuffle=True)

model = LSTM().cuda()
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.8)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)
```

- **æ¨¡å‹è®­ç»ƒï¼š**

```
train_loss = []
test_loss = []

for epoch in range(EPOCHS):
    model.train()
    for x, y in train_loader:
        var_x = x.cuda()
        var_y = y.cuda()

        out = model(var_x)

        loss = criterion(out, var_y)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    if epoch % 50 == 0:
        print(f'EPOCH:{epoch},Train Loss:{loss.item():.5f}')

    train_loss.append(loss.item())
    model.eval()
    with torch.no_grad():
        for x, y in test_loader:
            var_tx = x.cuda()
            var_ty = y.cuda()

            out = model(var_tx)
            loss = criterion(out, var_ty)
        test_loss.append(loss.item())
            
    if epoch % 50 == 0:
        print(f'Test Loss:{loss.item():.5f}')
    
    scheduler.step() # å­¦ä¹ ç‡é€’å‡
```

- **æŸ¥çœ‹é¢„æµ‹ç»“æœï¼š**

```
# losså˜åŒ–æ›²çº¿
plt.figure()
plt.plot(train_loss, label='train loss')
plt.plot(test_loss, label='test loss')
plt.legend()
plt.show()
```

![](./picture/loss.png)

- **é¢„æµ‹ç»“æœå¯è§†åŒ–ï¼š**

```
# è®­ç»ƒç»“æœå¯è§†åŒ–
train_preds = []
for x in tqdm(train_tensor):
    x = x.unsqueeze(dim=-1).unsqueeze(dim=0).cuda()
    preds = model(x)
    train_preds.append(preds.item())
plt.figure()
plt.plot(train_preds, label='preds')
plt.plot(train_labels, label='real')
plt.legend()
plt.show()
```

![](./picture/train_preds.png)

```
# æµ‹è¯•ç»“æœå¯è§†åŒ–
test_preds = []
for x in tqdm(test_tensor):
    x = x.unsqueeze(dim=-1).unsqueeze(dim=0).cuda()
    preds = model(x)
    test_preds.append(preds.item())
    
plt.figure()
plt.plot(test_preds, '.-', label='preds', alpha=0.7)
plt.plot(test_labels, '.-', label='real', alpha=0.7)
plt.legend()
plt.show()
```

![](./picture/test_preds.png)

### Part5 ç»“æœåˆ†æ

åœ¨æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨å‰7å¤©çš„æ•°æ®ï¼Œé¢„æµ‹åä¸€å¤©çš„æ•°æ®ã€‚ä»è®­ç»ƒç»“æœæ¥çœ‹ï¼Œé¢„æµ‹å€¼å¾ˆå¥½åœ°æ‹Ÿåˆäº†åŸæ¥çš„çœŸå®å€¼ã€‚

ä½†ä»”ç»†æŸ¥çœ‹ç»“æœï¼Œå¯ä»¥å‘ç°LSTMé¢„æµ‹ç¬¬äºŒå¤©çš„ç»“æœï¼Œåªæ˜¯å¾ˆæ¥è¿‘å‰ä¸€å¤©çš„å€¼ã€‚æ‰€ä»¥ï¼ŒLSTMå¹¶ä¸èƒ½å¾ˆå¥½åœ°é¢„æµ‹è¿™ç§é‡‘èæ•°æ®ï¼

æ‰€ä»¥å¯¹äºé‡‘èé¢†åŸŸï¼Œæ²¡æœ‰å¯é çš„ç®—æ³•æ˜¯å¯ä»¥é¢„æµ‹å…¶èµ°å‘çš„ï¼å› ä¸ºé‡‘èçš„äº¤æ˜“æ”¶åˆ°å¾ˆå¤šå› ç´ çš„å½±å“ï¼Œè¿˜æœ‰ä¸€äº›æ— å½¢çš„å› ç´ æ˜¯æ— æ³•äº‹å…ˆé¢„æµ‹çš„ï¼

**å‚è€ƒï¼š**

- pytorch æ¨¡å‹ä»£ç è®²è§£ï¼šhttps://mp.weixin.qq.com/s/Ge1rZszg4IiLMNo7Y0KDAw

> è¯»å®Œåå°±çŸ¥é“ä»£ç kè¾“å…¥æ˜¯ä»€ä¹ˆæ ·å­

- lstmå®ç°è‚¡ç¥¨é¢„æµ‹ï¼šhttps://github.com/TankZhouFirst/Pytorch-LSTM-Stock-Price-Predict

> çœ‹å®Œåå°±çŸ¥é“æ¨¡å‹è¦æ€ä¹ˆæ„å»º

- lstm æ¨¡å‹ç†è®ºè®²è§£ï¼šhttps://zhuanlan.zhihu.com/p/32085405

> çœ‹å®Œåèƒ½å¤Ÿäº†è§£æ¨¡å‹çš„ç†è®ºçŸ¥è¯†

- æœºå™¨ä¹‹å¿ƒï¼šhttps://www.jiqizhixin.com/articles/2019-01-04-16

> ä½¿ç”¨äº†å¤šä¸ªæ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼Œå¹¶å¯¹æ•ˆæœè¿›è¡Œäº†é¢„æµ‹ï¼Œä½†æ˜¯ä»£ç ç‰ˆæœ¬ä¸ä¸€æ ·

- æ·±åº¦å­¦ä¹ åšè‚¡ç¥¨é¢„æµ‹é è°±å—ï¼šhttps://www.zhihu.com/question/54542998

> æœ‰è¯´é è°±çš„ï¼Œä¹Ÿæœ‰è¯´ä¸é è°±çš„ï¼Œä½†æ˜¯ä¸é è°±çš„å¤šï¼Œçœ‹ä½ è‡ªå·±æ€ä¹ˆæƒ³

- kaggle å•†å“äº¤æ˜“é‡é¢„æµ‹ï¼šhttps://www.kaggle.com/zhangyunsheng/xgboost
    
  - æ¯”èµ›åœ°å€ï¼šhttps://www.kaggle.com/c/competitive-data-science-predict-future-sales/code
  - å‚è€ƒé¡¹ç›®ï¼šhttps://www.kaggle.com/yasserhessein/predict-future-sales-using-4-algorithms-regression
